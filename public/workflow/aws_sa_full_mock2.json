[
    {
        "q_number": "54",
        "q_text": "An ecommerce company has an order-processing application that uses Amazon API Gateway and an AWS Lambda function. The application stores data in an Amazon Aurora Postgre SQL database. During a recent sales event, a sudden surge in customer orders occurred. Some customers experienced timeouts, and the application did not process the orders of those customers. Asolutions architect determined that the CPU utilization and memory utilization were high on the database because of a large number of open connections. The solutions architect needs to prevent the time outerrors while making the least possible changes to the application. Which solution will meet these requirements?",
        "options": [
          {
            "A": "Use Amazon RDS Proxy to create a proxy for the database. Modify the Lambda function to use the RDS Proxy endpoint instead of the database endpoint."
          },
          {
            "B": "Create a read replica for the database in a different AWS Region. Use query string parameters in API Gateway to route traffic to the read replica."
          },
          {
            "C": "Configure provisioned concurrency for the Lambda function. Modify the database to be a global database in multiple AWS Regions."
          },
          {
            "D": "Migrate the data from Aurora Postgre SQL to Amazon Dynamo DB by using AWS Database Migration Service (AWSDMS). Modify the Lambda function to use the Dynamo DB table."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "55",
        "q_text": "A company uses Amazon S3 as its data lake. The company has a new partner that must use SFTP to upload data files. Asolutions architect needs to implement a highly available SFTP solution that minimizes operational overhead. Which solution will meet these requirements?",
        "options": [
          {
            "A": "Launch an Amazon EC2 instance in a private subnet in a VPC. Instruct the new partner to upload files to the EC2 instance by using a VPN. Run a cron job script, on the EC2 instance to upload files to the S3 data lake."
          },
          {
            "B": "Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible endpoint. Choose the S3 data lake as the destination."
          },
          {
            "C": "Use Amazon S3 File Gateway as an SFTP server. Expose the S3 File Gateway endpoint URL to the new partner. Share the S3 File Gateway endpoint with the new partner."
          },
          {
            "D": "Launch Amazon EC2 instances in a private subnet in a VPC. Place a Network Load Balancer (NLB) in front of the EC2 instances. Create an SFTP listener port for the NLB. Share the NLB hostname with the new partner. Run a cron job script on the EC2 instances to upload files to the S3 data lake."
          }
        ],
        "correct_answer": "B",
        "explanation": "The correct answer is B: Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible endpoint. Choose the S3 data lake as the destination. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "56",
        "q_text": "A company has multiple VPCs across AWS Regions to support and run workloads that are isolated from workloads in other Regions. Because of a recent application launch requirement, the company's VPCs must communicate with all other VPCs across all Regions. Which solution will meet these requirements with the LEAST amount of administrative effort?",
        "options": [
          {
            "A": "Use AWS Transit Gateway to manage VPC communication in a single Region and Transit Gateway peering across Regions to manage VPC communications."
          },
          {
            "B": "Use AWSPrivate Link across all Regions to connect VPCs across Regions and manage VPC communications"
          },
          {
            "C": "Use AWS Direct Connect gateways across all Regions to connect VPCs across regions and manage VPC communications."
          },
          {
            "D": "Use VPC peering to manage VPC communication in a single Region. Use VPC peering across Regions to manage VPC communications."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "57",
        "q_text": "A company has a web application for travel ticketing. The application is based on a database that runs in a single data center in North America. The company wants to expand the application to serve a global user base. The company needs to deploy the application to multiple AWS Regions. Average latency must be less than 1 second on updates to the reservation database. The company wants to have separate deployment solutions for its web platform across multiple Regions. However, the company must maintain a single primary reservation database that is globally consistent. Which solutionshouldasolutionsarchitectrecommendtomeettheserequirements?",
        "options": [
          {
            "A": "Migrate the application to an Amazon Aurora Serverless database. Deploy instances of the database to each Region. Use the correct Regional endpoint in each Regional deployment to access the database. Use AWS Lambda functions to process event streams in each Region to synchronize the databases."
          },
          {
            "B": "Convert the application to use Amazon Dynamo DB. Use a global table for the center reservation table. Use the correct Regional endpoint in each Regional deployment."
          },
          {
            "C": "Migrate the database to an Amazon Aurora MySQL database. Deploy Aurora Read Replicas in each Region. Use the correct Regional endpoint in each Regional deployment for access to the database."
          },
          {
            "D": "Migrate the database to an Amazon RDS for MySQL database. Deploy MySQL read replicas in each Region. Use the correct Regional endpoint in each Regional deployment for access to the database."
          }
        ],
        "correct_answer": "B",
        "explanation": "The correct answer is B: Convert the application to use Amazon Dynamo DB. Use a global table for the center reservation table. Use the correct Regional endpoint in each Regional deployment. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "58",
        "q_text": "A company has multiple Windows file servers on premises. The company wants to migrate and consolidate its files into an Amazon FSx for Windows File Server file system. File permissions must be preserved to ensure that access rights do not change. Which solutions will meet these requirements? (Choosetwo.)",
        "options": [
          {
            "A": "Order an AWS Snowball Edge Storage Optimized device. Connect the device to the on-premises network. Copy data to the device by using the AWS CLI. Ship the device back to AWS for import into Amazon S3. Schedule AWS Data Sync tasks to transfer the data to the FSx for Windows File Server filesystem."
          },
          {
            "B": "Deploy AWS Data Sync agents on premises. Schedule Data Sync tasks to transfer the data to the FSx for Windows File Server filesystem. Correct selection"
          },
          {
            "C": "Remove the drives from each file server. Ship the drives to AWS for import into Amazon S3. Schedule AWS Data Sync tasks to transfer the data to the FSx for Windows File Server filesystem."
          },
          {
            "D": "Order an AWS Snowconedevice. Connect the device to the on-premises network. Launch AWS Data Sync agents on the device. Schedule Data Sync tasks to transfer the data to the FSx for Windows File Server filesystem."
          },
          {
            "E": "Copy the shares on each file server into Amazon S3 buckets by using the AWS CLI. Schedule AWS Data Sync tasks to transfer the data to the FSx for Windows File Server filesystem."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "59",
        "q_text": "A company's website provides users with downloadable historical performance reports. The website needs a solution that will scale to meet the company's website demands globally. The solution should be cost-effective, limit the provisioning of infrastructure resources, and provide the fastest possible response time. Which combinations should a solutions architect recommend to meet these requirements?",
        "options": [
          {
            "A": "Application Load Balancer with Amazon EC2 Auto Scaling"
          },
          {
            "B": "Amazon CloudFront and Amazon S3"
          },
          {
            "C": "AWS Lambda and Amazon Dynamo DB"
          },
          {
            "D": "Amazon Route 53 withinternal Application Load Balancers"
          }
        ]
      },
      {
        "q_number": "11",
        "q_text": "A company provides an API to its users that automates inquiries for tax computations based on item prices. The company experiences a large number of inquiries during the holiday season only that causes lower response times. A solutions architect needs to design a solution that is scalable and elastic. What should the solutions architect do to accomplish this?",
        "options": [
          {
            "A": "Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon EC2 instance. API Gateway accepts and passes the item names to the EC2 instance for tax computations."
          },
          {
            "B": "Design a REST API using Amazon API Gateway that accepts the item names. API Gateway passes item names to AWS Lambda for tax computations. Correct answer"
          },
          {
            "C": "Create an Application Load Balancer that has two Amazon EC2 instances behind it. The EC2 instances will compute the tax on the received item names."
          },
          {
            "D": "Provide an API hosted on an Amazon EC2 instance. The EC2 instance performs the required computations when the API request is made."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "12",
        "q_text": "A company has a service that reads and writes large amounts of data from an Amazon S3 bucket in the same AWS Region. The service is deployed on Amazon EC2 instances within the private subnet of a VPC. The service communicates with Amazon S3 over a NAT gateway in the public subnet. However, the company wants a solution that will reduce the data output costs. Which solution will meet theserequirements MOST cost-effectively?",
        "options": [
          {
            "A": "Provision a dedicated EC2 NAT instance in the private subnet. Configure the route table for the public subnet to use the elastic network interface of this instance as the destination for all S3 traffic."
          },
          {
            "B": "Provision a VPC gateway endpoint. Configure the route table for the private subnet to use the gateway endpoint as the route for all S3 traffic."
          },
          {
            "C": "Provision a second NAT gateway. Configure the route table for the private subnet to use this NAT gateway as the destination for all S3 traffic."
          },
          {
            "D": "Provision a dedicated EC2 NAT instance in the public subnet. Configure the route table for the private subnet to use the elastic network interface of this instance as the destination for all S3 traffic."
          }
        ],
        "correct_answer": "B",
        "explanation": "The correct answer is B: Provision a VPC gateway endpoint. Configure the routetable for the private subnettousethegatewayendpointastherouteforall S 3 traffic. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "13",
        "q_text": "A solutions architect needs to securely store a database username and password that an application uses to access an Amazon RDS DB instance. The application that accesses the database runs on an Amazon EC2 instance. The solutions architect want to create a secure parameter in AWS Systems Manager Parameter Store. What should the solutions architect do to meet this requirement?",
        "options": [
          {
            "A": "Create an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM role to the EC2 instance. Correct answer"
          },
          {
            "B": "Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance. Specify Amazon RDS as a principal in the trust policy."
          },
          {
            "C": "Create an IAM trust relationship between the DB instance and the EC2 instance. Specify Systems Manager as a principal in the trust policy."
          },
          {
            "D": "Create an IAM policy that allows read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM policy to the EC2 instance."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "14",
        "q_text": "A solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the information submitted by users is sensitive. The application uses HTTPS but needs another layer of security. The sensitive information should be protected throughout the entire application stack, and access to the information should be restricted to certain applications. Which action should the solutions architect take?",
        "options": [
          {
            "A": "Configure a CloudFront field-level encryption profile."
          },
          {
            "B": "Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol Policy."
          },
          {
            "C": "Configure a CloudFront signed cookie."
          },
          {
            "D": "Configure a CloudFront signed URL."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is A: Configure a CloudFront field-level encryption profile. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "15",
        "q_text": "A company has a legacy data processing application that runs on Amazon EC2 instances. Data is processed sequentially, but the order of results does not matter. The application uses a monolithic architecture. The only way that the company can scale the application to meet increased demand is to increase the size of the instances. The company's developers have decided to rewrite the application to use a microservices architecture on Amazon Elastic Container Service (Amazon ECS). What should a solutions architect recommend for communication between the microservices?",
        "options": [
          {
            "A": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Add code to the data producers, and publish notifications to the topic. Add code to the data consumers to subscribe to the topic."
          },
          {
            "B": "Create an Amazon Dynamo DB table. Enable Dynamo DB Streams. Add code to the data producers to insert data into the table. Add code to the data consumers to use the Dynamo DB Streams API to detect new table entries and retrieve the data."
          },
          {
            "C": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Add code to the data producers, and send data to the queue. Add code to the data consumers to process data from the queue."
          },
          {
            "D": "Create an AWS Lambda function to pass messages. Add code to the data producers to call the Lambda function with a data object. Add code to the data consumers to receive a data object that is passed from the Lambda function."
          }
        ],
        "correct_answer": "C",
        "explanation": "The correct answer is C: Create an Amazon Simple Queue Service (Amazon SQS) queue. Add code to the data producers, and send data to the queue. Add code to the data consumers to process data from the queue. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "16",
        "q_text": "A company seeks a storage solution for its application. The solution must be highly available and scalable. The solution also must function as a file system, be mountable by multiple Linux instances in AWS and on-premises through native protocols, and have minimum size requirements. The company has set up a Site-to-Site VPN for access from its on-premises network to its VPC. Which storage solution meets these requirements?",
        "options": [
          {
            "C": "Which storage solution meets these requirements?"
          },
          {
            "A": "Amazon FSx Multi-AZ deployments"
          },
          {
            "B": "Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes"
          },
          {
            "C": "Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points"
          },
          {
            "D": "Amazon Elastic File System (Amazon EFS) with multiple mount targets Correct answer"
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "17",
        "q_text": "A company is developing an e-commerce application that will consist of a load-balanced frontend, a container-based application, and a relational database. A solutions architect needs to create a highly available solution that operates with as little manual intervention as possible. Which solutions meet these requirements? (Choosetwo.)",
        "options": [
          {
            "A": "Create an Amazon RDS DB instance in Multi-AZ mode. Correct selection"
          },
          {
            "B": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type to handle the dynamic application load. Correct selection"
          },
          {
            "C": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type to handle the dynamic application load."
          },
          {
            "D": "Create an Amazon RDS DB instance and one or more replicas in another Availability Zone."
          },
          {
            "E": "Create an Amazon EC2 instance-based Docker cluster to handle the dynamic application load."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "18",
        "q_text": "A company has an ordering application that stores customer information in Amazon RDS for MySQL. During regular business hours, employees run one-time queries for reporting purposes. Timeouts are occurring during order processing because the reporting queries are taking a long time to run. The company needs to solve the timeouts without preventing employees from performing queries. What should a solutions architect do to meet these requirements?",
        "options": [
          {
            "A": "Create a read replica. Distribute the ordering application to the primary DB instance and the read replica."
          },
          {
            "B": "Schedule the reporting queries for non-peak hours."
          },
          {
            "C": "Create a read replica. Move reporting queries to the read replica."
          },
          {
            "D": "Migrate the ordering application to Amazon Dynamo DB with on-demand capacity."
          }
        ],
        "correct_answer": "C",
        "explanation": "The correct answer is C: Create a read replica. Move reporting queries to the read replica.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "19",
        "q_text": "A company has a service that reads and writes large amounts of data from an Amazon S3 bucket in the same AWS Region. The service is deployed on Amazon EC2 instances within the private subnet of a VPC. The service communicates with Amazon S3 over a NAT gateway in the public subnet. However, the company wants a solution that will reduce the data output costs. Which solution will meet theserequirements MOST cost-effectively?",
        "options": [
          {
            "A": "Provision a dedicated EC2 NAT instance in the private subnet. Configure the route table for the public subnet to use the elastic network interface of this instance as the destination for all S3 traffic."
          },
          {
            "B": "Provision a VPC gateway endpoint. Configure the route table for the private subnet to use the gateway endpoint as the route for all S3 traffic."
          },
          {
            "C": "Provision a second NAT gateway. Configure the route table for the private subnet to use this NAT gateway as the destination for all S3 traffic."
          },
          {
            "D": "Provision a dedicated EC2 NAT instance in the public subnet. Configure the route table for the private subnet to use the elastic network interface of this instance as the destination for all S3 traffic."
          }
        ],
        "correct_answer": "B",
        "explanation": "The correct answer is B: Provision a VPC gateway endpoint. Configure the routetable for the private subnettousethegatewayendpointastherouteforall S 3 traffic.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "20",
        "q_text": "A company has Amazon EC2 instances that run nightly batch jobs to process data. The EC2 instances run in an Auto Scaling group that uses On-Demand billing. If a job fails on one instance, another instance will reprocess the job. The batch jobs run between 12:00 AM and 06:00 AM local time every day. Which solution will provide EC2 instances to meet these requirements MOST cost-effectively?",
        "options": [
          {
            "A": "Purchase a 1-year Reserved Instance for the specific instance type and operating system of the instances in the Auto Scaling group that the batch job uses."
          },
          {
            "B": "Purchase a 1-year Savings Plan for Amazon EC2 that covers the instance family of the Auto Scaling group that the batch job uses."
          },
          {
            "C": "Create a new launch template for the Auto Scaling group. Set the instances to Spot Instances. Set a policy to scale out based on CPU usage."
          },
          {
            "D": "Create a new launch template for the Auto Scaling group. Increase the instance size. Set a policy to scale out based on CPU usage."
          }
        ],
        "correct_answer": "C",
        "explanation": "The correct answer is C: Create a new launch template for the Auto Scaling group. Set the instances to Spot Instances. Set a policy to scale out based on CPU usage. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "21",
        "q_text": "A company's web application is running on Amazon EC2 instances behind an Application Load Balancer. The company recently changed its policy, which now requires the application to be accessed from one specific country only. Which configuration will meet this requirement?",
        "options": [
          {
            "A": "Configure the security group for the EC2 instances."
          },
          {
            "B": "Configure AWS WAF on the Application Load Balancer in a VPC."
          },
          {
            "C": "Configure the network ACL for the subnet that contains the EC2 instances."
          },
          {
            "D": "Configure the security group on the Application Load Balancer."
          },
          {
            "C": "AWS WAF allows you to create a web access control list (web ACL) that defines rules for allowing or blocking web requests based on various conditions, including the country or region from which the request is originating. You can associate this AWS WAF web ACL with your Application Load Balancer (ALB) in your VPC. This will enable AWS WAF to inspect incoming web requests and block any requests that do not meet the defined criteria, such as requests originating from outside the specific country. This approach provides a more targeted and flexible way to restrict access to your application based on geographic location while allowing legitimate traffic to reach your EC2 instances behind the ALB."
          }
        ],
        "correct_answer": "B",
        "explanation": "The correct answer is B: Configure AWS WAF on the Application Load Balancer in a VPC.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "24",
        "q_text": "A company runs an infrastructure monitoring service. The company is building a new feature that will enable the service to monitor data in customer AWS accounts. The new feature will call AWS APIs in customer accounts to describe Amazon EC2 instances and read Amazon Cloud Watch metrics. What should the company do to obtain access to customer accounts in the MOST secure way?",
        "options": [
          {
            "A": "Ensure that the customers create an Amazon Cognito user in their account to use an IAM role with read-only EC2 and Cloud Watch permissions. Encrypt and store the Amazon Cognito user and password in a secrets management system."
          },
          {
            "B": "Ensure that the customers create an IAM user in their account with read-only EC2 and Cloud Watch permissions. Encrypt and store customer access and secret keys in a secrets management system."
          },
          {
            "C": "Create a serverless API that implements a token vending machine to provide temporary AWS credentials for a role with read-only EC2 and Cloud Watch permissions."
          },
          {
            "D": "Ensure that the customers create an IAM role in their account with read-only EC2 and Cloud Watch permissions and a trust policy to the company's account."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "25",
        "q_text": "A company runs a web-based portal that provides users with global breaking news, local alerts, and weather updates. The portal delivers each user a personalized view by using a mixture of static and dynamic content. Content is served over HTTPS through an API server running on an Amazon EC2 instance behind an Application Load Balancer (ALB). The company wants the portal to provide this content to its users across the world as quickly as possible. How should a solutions architect design the application to ensure the LEAST amount of latency for all users?",
        "options": [
          {
            "A": "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 geolocation routing policy to serve all content from the ALB in the closest Region."
          },
          {
            "B": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin. Correct answer"
          },
          {
            "C": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve the static content. Serve the dynamic content directly from the ALB."
          },
          {
            "D": "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest Region."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "27",
        "q_text": "A business application is hosted on Amazon EC2 and uses Amazon S3 for encrypted object storage. The chief information security officer has directed that no application traffic between the two services should traverse the public internet. Which capability should the solutions architect use to meet the compliance requirements?",
        "options": [
          {
            "A": "VPC endpoint"
          },
          {
            "B": "AWS Key Management Service (AWSKMS)"
          },
          {
            "C": "Private subnet"
          },
          {
            "D": "Virtual private gateway"
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is A: VPC endpoint. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "28",
        "q_text": "An application runs on Amazon EC2 instances in private subnets. The application needs to access an Amazon Dynamo DB table. What is the MOST secure way to access the table while ensuring that the traffic does not leave the AWS network?",
        "options": [
          {
            "A": "Use a VPC endpoint for Dynamo DB."
          },
          {
            "B": "Use a NAT gateway in a public subnet."
          },
          {
            "C": "Use the internet gateway attached to the VPC."
          },
          {
            "D": "Use a NAT instance in a private subnet."
          },
          {
            "B": "Explanation: The most secure way to access an Amazon Dynamo DB table from Amazon EC2 instances in private subnets while ensuring that the traffic does not leave the AWS network is to use a VPC endpoint for Dynamo DB. Here's why: 1. VPCEndpoints: A VPC endpoint allows you to privately connect your VPC to supported AWS services like Dynamo DB without the need for an internet gateway, NAT gateway, or NAT instance. It keeps the traffic within the AWS network and does not traverse the public internet. 2. Security: Using a VPC endpoint provides a more secure connection as it doesn't rely on public internet access. It also eliminates the need to configure complex network address translation (NAT) gateways or instances. 3. Simplicity: It is a simpler and more direct method of accessing Dynamo DB without exposing your resources to the internet. This reduces the attack surface and ensures that the traffic remains within your VPC."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is A: Use a VPC endpoint for Dynamo DB.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "29",
        "q_text": "A company uses a three-tier web application to provide training to new employees. The application is accessed for only 12 hours each day. The company is using an Amazon RDS for MySQL DB instance to store information and wants to minimize costs. What should a solutions architect do to meet these requirements?",
        "options": [
          {
            "A": "Create AWS Lambda functions to start and stop the DB instance. Create Amazon Event Bridge (Amazon Cloud Watch Events) scheduled rules to invoke the Lambda functions. Configure the Lambda functions as event targets for the rules. Correct answer"
          },
          {
            "B": "Launch an Amazon EC2 instance. Create an IAM role that grants access to Amazon RDS. Attach the role to the EC2 instance. Configure a cron job to start and stop the EC2 instance on the desired schedule."
          },
          {
            "C": "Configure an IAM policy for AWS Systems Manager Session Manager. Create an IAM role for the policy. Update the trust relationship of the role. Setup automatic start and stop for the DB instance."
          },
          {
            "D": "Create an Amazon ElastiCache for Redis cache cluster that gives users the ability to access the data from the cache when the DB instance is stopped. Invalidate the cache after the DB instance is started."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "30",
        "q_text": "A company needs to run a critical application on AWS. The company needs to use Amazon EC2 for the application's database. The database must be highly available and must failover automatically if a disruptive event occurs. Which solution will meet theserequirements?",
        "options": [
          {
            "A": "Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to backup the data. Use EC2 automatic recovery to recover the instance if a disruptive event occurs."
          },
          {
            "B": "Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to backup the data. Use AWS Cloud Formation to automate provisioning of the EC2 instance if a disruptive event occurs."
          },
          {
            "C": "Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Setup database replication."
          },
          {
            "D": "Launch two EC2 instances, each in a different AWS Region. Install the database on both EC2 instances. Setup database replication. Failover the database to a second Region."
          }
        ],
        "correct_answer": "C",
        "explanation": "The correct answer is C: Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Setup database replication.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS."
      },
      {
        "q_number": "31",
        "q_text": "A company uses Amazon S3 to store high-resolution pictures in an S3 bucket. To minimize application changes, the company stores the pictures as the latest version of an S3 object. The company needs to retain only the two most recent versions of the pictures. The company wants to reduce costs. The company has identified the S3 bucket as a large expense. Which solution will reduce the S3 costs with the LEAST operational overhead?",
        "options": [
          {
            "A": "Use S3 Lifecycle to delete expired object versions and retain the two most recent versions."
          },
          {
            "B": "Deactivate versioning on the S3 bucket and retain the two most recent versions."
          },
          {
            "C": "Use an AWS Lambda function to check for older versions and delete all but the two most recent versions."
          },
          {
            "D": "Use S3 Batch Operations to delete non-current object versions and retain only the two most recent versions."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is A: Use S3 Lifecycle to delete expired object versions and retain the two most recent versions. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "32",
        "q_text": "A company wants to migrate 100 GB of historical data from an on-premises location to an Amazon S3 bucket. The company has a 100 megabits per second (Mbps) internet connection on-premises. The company needs to encrypt the data in transit to the S3 bucket. The company will store new data directly in Amazon S3. Which solution will meet these requirements with the LEAST operational overhead?",
        "options": [
          {
            "A": "Setup an IPsec VPN from the on-premises location to AWS. Use the s3 cp command in the AWS CLI to move the data directly to an S3 bucket"
          },
          {
            "B": "Use AWS Data Sync to migrate the data from the on-premises location to an S3 bucket"
          },
          {
            "C": "Use the s3 sync command in the AWS CLI to move the data directly to an S3 bucket"
          },
          {
            "D": "Use AWS Snowball to move the data to an S3 bucket"
          }
        ],
        "correct_answer": "B",
        "explanation": "The correct answer is B: Use AWS Data Sync to migrate the data from the on-premises location to an S3 bucket. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "33",
        "q_text": "A company has two applications: a sender application that sends messages with payloads to be processed and a processing application intended to receive the messages with payloads. The company wants to implement an AWS service to handle messages between the two applications. The sender application can send about 1,000 messages each hour. The messages may take up to 2 days to be processed: If the messages fail to process, they must be retained so that they do not impact the processing of any remaining messages. Which solution meets these requirements and is the MOST operationally efficient?",
        "options": [
          {
            "A": "Setup an Amazon EC2 instance running a Redis database. Configure both applications to use the instance. Store, process, and delete the messages, respectively."
          },
          {
            "B": "Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications to process. Integrate the sender application to write to the SNS topic."
          },
          {
            "C": "Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon SQS) queue. Configure a dead-letter queue to collect the messages that failed to process."
          },
          {
            "D": "Use an Amazon Kinesis data stream to receive the messages from the sender application. Integrate the processing application with the Kinesis Client Library (KCL)."
          }
        ],
        "correct_answer": "C",
        "explanation": "The correct answer is C: Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon SQS) queue. Configure a dead-letter queue to collect the messages that failed to process.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "34",
        "q_text": "A company has a small Python application that processes JSON documents and outputs the results to an on-premises SQL database. The application runs thousands of times each day. The company wants to move the application to the AWS Cloud. The company needs a highly available solution that maximizes scalability and minimizes operational overhead. Which solution will meet these requirements?",
        "options": [
          {
            "A": "Place the JSON documents in an Amazon S3 bucket. Run the Python code on multiple Amazon EC2 instances to process the documents. Store the results in an Amazon Aurora DB cluster."
          },
          {
            "B": "Place the JSON documents in an Amazon Simple Queue Service (Amazon SQS) queue as messages. Deploy the Python code as a container on an Amazon Elastic Container Service (Amazon ECS) cluster that is configured with the Amazon EC2 launch type. Use the container to process the SQS messages. Store the results on an Amazon RDS DB instance."
          },
          {
            "C": "Place the JSON documents in an Amazon S3 bucket. Create an AWS Lambda function that runs the Python code to process the documents as they arrive in the S3 bucket. Store the results in an Amazon Aurora DB cluster."
          },
          {
            "D": "Place the JSON documents in an Amazon Elastic Block Store (Amazon EBS) volume. Use the EBS Multi-Attach feature to attach the volume to multiple Amazon EC2 instances. Run the Python code on the EC2 instances to process the documents. Store the results on an Amazon RDS DB instance."
          }
        ],
        "correct_answer": "C",
        "explanation": "The correct answer is C: Place the JSON documents in an Amazon S3 bucket. Create an AWS Lambda function that runs the Python code to process the documents as they arrive in the S3 bucket. Store the results in an Amazon Aurora DB cluster.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "35",
        "q_text": "A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon Quick Sight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket. The reporting team wants to move the files automatically to the analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon Sage Maker Pipelines. What should a solutions architect do to meet these requirements with the LEAST operational overhead?",
        "options": [
          {
            "A": "Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon Event Bridge (Amazon Cloud Watch Events). Configure an Object Created rule in Event Bridge (Cloud Watch Events). Configure Lambda and Sage Maker Pipelines as targets for the rule."
          },
          {
            "B": "Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and Sage Maker Pipelines as destinations of the event notification. Configure s3:Object Created:Put as the event type."
          },
          {
            "C": "Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon Event Bridge (Amazon Cloud Watch Events). Configure an Object Created rule in Event Bridge (Cloud Watch Events). Configure Lambda and Sage Maker Pipelines as targets for the rule."
          },
          {
            "D": "Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and Sage Maker Pipelines as destinations of the event notification. Configure s3:Object Created:Put as the event type."
          }
        ],
        "correct_answer": "C",
        "explanation": "The correct answer is C: Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon Event Bridge (Amazon Cloud Watch Events). Configure an Object Created rule in Event Bridge (Cloud Watch Events). Configure Lambda and Sage Maker Pipelines as targets for the rule.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "36",
        "q_text": "A company's orders system sends requests from clients to Amazon EC2 instances. The EC2 instances process the orders and then store the orders in a database on Amazon RDS. Users report that they must reprocess orders when the system fails. The company wants a resilient solution that can process orders automatically if a system outage occurs. What should a solutions architect do to meet these requirements?",
        "options": [
          {
            "A": "Move the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB). Update the orders system to send messages to the ALB endpoint."
          },
          {
            "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function, and subscribe the function to the SNS topic. Configure the orders system to send messages to the SNS topic. Send a command to the EC2 instances to process the messages by using AWS Systems Manager Run Command."
          },
          {
            "C": "Move the EC2 instances into an Auto Scaling group. Create an Amazon Event Bridge (Amazon Cloud Watch Events) rule to target an Amazon Elastic Container Service (Amazon ECS) task."
          },
          {
            "D": "Move the EC2 instances into an Auto Scaling group. Configure the orders system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the EC2 instances to consume messages from the queue. Correct answer"
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "37",
        "q_text": "A company wants to migrate its MySQL database from on-premises to AWS. The company recently experienced a database outage that significantly impacted the business. To ensure this doesn't happen again, the company wants a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes. Which solution meets these requirements?",
        "options": [
          {
            "A": "Create an Amazon EC2 instance with a MySQL engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance."
          },
          {
            "B": "Create an Amazon RDS DB instance with synchronous replication to three nodes in three Availability Zones."
          },
          {
            "C": "Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data."
          },
          {
            "D": "Create an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data."
          }
        ],
        "correct_answer": "C",
        "explanation": "The correct answer is C: Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "38",
        "q_text": "A company's infrastructure consists of Amazon EC2 instances and an Amazon RDS DB instance in a single AWS Region. The company wants to backup its data in a separate Region. Which solution will meet these requirements with the LEAST operational overhead?",
        "options": [
          {
            "A": "Use AWS Backup to copy EC2 backups and RDS backups to the separate Region."
          },
          {
            "B": "Create Amazon Machine Images (AMIs) of the EC2 instances. Copy the AMI to the separate Region. Create a read replica for the RDS DB instance in the separate Region."
          },
          {
            "C": "Use Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 backups and RDS backups to the separate Region."
          },
          {
            "D": "Create Amazon Elastic Block Store (Amazon EBS) snapshots. Copy the EBS snapshots to the separate Region. Create RDS snapshots. Export the RDS snapshots to Amazon S3. Configure S3 Cross-Region Replication (CRR) to the separate Region."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "39",
        "q_text": "A company is expecting rapid growth in the near future. A solutions architect needs to configure existing users and grant permissions to new users on AWS. The solutions architect has decided to create IAM groups. The solutions architect will add the new users to IAM groups based on department. Which additional action is the MOST secure way to grant permissions to the new users?",
        "options": [
          {
            "A": "Create IAM roles. Associate the roles with a permissions boundary that defines the maximum permissions"
          },
          {
            "B": "Create IAM roles that have least privilege permission. Attach the roles to the IAM groups"
          },
          {
            "C": "Apply service control policies (SCPs) to manage access permissions"
          },
          {
            "D": "Create an IAM policy that grants least privilege permission. Attach the policy to the IAM groups"
          }
        ],
        "correct_answer": "D",
        "explanation": "The correct answer is D: Create an IAMpolicythatgrantsleastprivilegepermission. Attachthepolicyto the IAMgroups. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "40",
        "q_text": "A company is concerned about the security of its public web application due to recent web attacks. The application uses an Application Load Balancer (ALB). A solutions architect must reduce the risk of DDoS attacks against the application. What should the solutions architect do to meet this requirement?",
        "options": [
          {
            "A": "Enable AWS Shield Advanced to prevent attacks."
          },
          {
            "B": "Configure Amazon Macie to prevent attacks."
          },
          {
            "C": "Configure Amazon Guard Duty to monitor the ALB."
          },
          {
            "D": "Add an Amazon Inspector agent to the ALB."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is A: Enable AWSShield Advanced to prevent attacks. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "41",
        "q_text": "A company has a mobile chat application with a data store based in Amazon Dynamo DB. Users would like new messages to be read with as little latency as possible. A solutions architect needs to design an optimal solution that requires minimal application changes. Which method should the solutions architect select?",
        "options": [
          {
            "A": "Add an Amazon ElastiCache for Rediscache to the application stack. Update the application to point to the Rediscache endpoint instead of Dynamo DB."
          },
          {
            "B": "Configure Amazon Dynamo DB Accelerator (DAX) for the new messages table. Update the code to use the DAX endpoint."
          },
          {
            "C": "Add Dynamo DB read replicas to handle the increased read load. Update the application to point to the read endpoint for the read replicas."
          },
          {
            "D": "Double the number of read capacity units for the new messages table in Dynamo DB. Continue to use the existing Dynamo DB endpoint."
          }
        ],
        "correct_answer": "B",
        "explanation": "The correct answer is B: Configure Amazon Dynamo DB Accelerator(DAX)forthenewmessagestable. Updatethecodetousethe DAXendpoint.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "42",
        "q_text": "A company has a service that produces event data. The company wants to use AWS to process the event data as it is received. The data is written in a specific order that must be maintained throughout processing. The company wants to implement a solution that minimizes operational overhead. How should a solutions architect accomplish this?",
        "options": [
          {
            "A": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing payloads to process. Configure an AWS Lambda function as a subscriber."
          },
          {
            "B": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages. Setup an AWS Lambda function to process messages from the queue independently."
          },
          {
            "C": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing payloads to process. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a subscriber."
          },
          {
            "D": "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages. Setup an AWS Lambda function to process messages from the queue."
          }
        ],
        "correct_answer": "D",
        "explanation": "The correct answer is D: Create an Amazon Simple Queue Service(Amazon SQS)FIFOqueuetohold messages. Setupan AWSLambdafunctiontoprocessmessagesfromthe queue.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "43",
        "q_text": "A solutions architect needs to help a company optimize the cost of running an application on AWS. The application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for compute within the architecture. The EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic and unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The application frontend will run on Fargate, and Lambda will serve the API layer. The front-end utilization and API layer utilization will be predictable over the course of the next year. Which combination of purchasing options will provide the MOST cost-effective solution for hosting this application? (Choose two.)",
        "options": [
          {
            "A": "Purchase 1-year All Upfront Reserved instances for the data ingestion layer."
          },
          {
            "B": "Purchase a 1-year EC2 instance Savings Plan for the frontend and API layer."
          },
          {
            "C": "Purchase a 1-year Compute Savings Plan for the frontend and API layer. Correct selection"
          },
          {
            "D": "Use Spot Instances for the data ingestion layer. Correct selection"
          },
          {
            "E": "Use On-Demand Instances for the data ingestion layer"
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "44",
        "q_text": "A company needs to minimize the cost of its 1 Gbps AWS Direct Connect connection. The company's average connection utilization is less than 10%. A solutions architect must recommend a solution that will reduce the cost without compromising security. Which solution will meet these requirements?",
        "options": [
          {
            "A": "Contact an AWS Direct Connect Partner to order a 1 Gbps connection. Share the connection with another AWS account."
          },
          {
            "B": "Contact an AWS Direct Connect Partner to order a 200 Mbps hosted connection for an existing AWS account."
          },
          {
            "C": "Set up a new 1 Gbps Direct Connect connection. Share the connection with another AWS account."
          },
          {
            "D": "Set up a new 200 Mbps Direct Connect connection in the AWS Management Console."
          }
        ],
        "correct_answer": "D",
        "explanation": "The correct answer is D: Setupanew 200 Mbps Direct Connectconnectioninthe AWSManagement Console.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "45",
        "q_text": "A company needs to save the results from a medical trial to an Amazon S3 repository. The repository must allow a few scientists to add new files and must restrict all other users to read-only access. No users can have the ability to modify or delete any files in the repository. The company must keep every file in the repository for a minimum of 1 year after its creation date. Which solution will meet these requirements?",
        "options": [
          {
            "A": "Use S3 Object Locking governance mode with a legal hold of 1 year."
          },
          {
            "B": "Configure the S3 bucket to invoke an AWS Lambda function every time an object is added. Configure the function to track the hash of the saved objects so that modified objects can be marked accordingly."
          },
          {
            "C": "Use S3 Object Lock in compliance mode with a retention period of 365 days."
          },
          {
            "D": "Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket. Use an S3 bucket policy to only allow the IAM role."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is e: . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "46",
        "q_text": "A company operates a two-tier application for image processing. The application uses two Availability Zones, each with one public subnet and one private subnet. An Application Load Balancer (ALB) for the web tier uses the public subnets. Amazon EC2 instances for the application tier use the private subnets. Users report that the application is running more slowly than expected. A security audit of the web server log files shows that the application is receiving millions of illegitimate requests from a small number of IP addresses. A solutions architect needs to resolve the immediate performance problem while the company investigates a more permanent solution. What should the solutions architect recommend to meet this requirement?",
        "options": [
          {
            "A": "Modify the network ACL for the web tier subnets. Add an inbound deny rule for the IP addresses that are consuming resources."
          },
          {
            "B": "Modify the network ACL for the application tier subnets. Add an inbound deny rule for the IP addresses that are consuming resources."
          },
          {
            "C": "Modify the inbound security group for the application tier. Add a deny rule for the IP addresses that are consuming resources."
          },
          {
            "D": "Modify the inbound security group for the web tier. Add a deny rule for the IP addresses that are consuming resources."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is A: Modifythenetwork ACLforthewebtiersubnets. Addaninbounddenyrulefor the IPaddressesthatareconsumingresources.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "47",
        "q_text": "A company has a web application that is based on Java and PHP. The company plans to move the application from on-premises to AWS. The company needs the ability to test new site features frequently. The company also needs a highly available and managed solution that requires minimum operational overhead. Which solution will meet these requirements?",
        "options": [
          {
            "A": "Containerize the web application. Deploy the web application to Amazon EC2 instances. Use the AWS Load Balancer Controller to dynamically route traffic between containers that contain the new site features for testing."
          },
          {
            "B": "Create an Amazon S3 bucket. Enable static web hosting on the S3 bucket. Upload the static content to the S3 bucket. Use AWS Lambda to process all dynamic content."
          },
          {
            "C": "Deploy the web application to Amazon EC2 instances that are configured with Java and PHP. Use Auto Scaling groups and an Application Load Balancer to manage the website's availability."
          },
          {
            "D": "Deploy the web application to an AWS Elastic Beanstalk environment. Use URL swapping to switch between multiple Elastic Beanstalk environments for feature testing."
          }
        ],
        "correct_answer": "D",
        "explanation": "The correct answer is D: Deploythewebapplicationto an AWSElastic Beanstalkenvironment. Use URL swappingtoswitchbetweenmultiple Elastic Beanstalkenvironmentsforfeature testing.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "48",
        "q_text": "An entertainment company is using Amazon Dynamo DB to store media metadata. The application is read intensive and experiencing delays. The company does not have staff to handle additional operational overhead and needs to improve the performance efficiency of Dynamo DB without reconfiguring the application. What should a solutions architect recommend to meet this requirement?",
        "options": [
          {
            "A": "Use Amazon ElastiCache for Memcached with Auto Discovery enabled."
          },
          {
            "B": "Use Amazon ElastiCache for Redis."
          },
          {
            "C": "Replicate data by using Dynamo DB global tables."
          },
          {
            "D": "Use Amazon Dynamo DB Accelerator (DAX)."
          }
        ],
        "correct_answer": "D",
        "explanation": "The correct answer is D: Use Amazon Dynamo DB Accelerator (DAX). This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "49",
        "q_text": "A company produces batch data that comes from different databases. The company also produces live stream data from network sensors and application APIs. The company needs to consolidate all the data into one place for business analytics. The company needs to process the incoming data and then stage the data in different Amazon S3 buckets. Teams will later run one-time queries and import the data into a business intelligence tool to show key performance indicators (KPIs). Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
        "options": [
          {
            "A": "Use Amazon Kinesis Data Analytics for one-time queries. Use Amazon QuickSight to create dashboards for KPIs."
          },
          {
            "B": "Use an AWS Glue extract, transform, and load (ETL) job to convert the data into JSON format. Load the data into multiple Amazon Open Search Service (Amazon Elasticsearch Service) clusters."
          },
          {
            "C": "Create custom AWS Lambda functions to move the individual records from the databases to an Amazon Redshift cluster."
          },
          {
            "D": "Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format. Correct selection"
          },
          {
            "E": "Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs. Correct selection"
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "50",
        "q_text": "A company is developing a file-sharing application that will use an Amazon S3 bucket for storage. The company wants to serve all the files through an Amazon CloudFront distribution. The company does not want the files to be accessible through direct navigation to the S3 URL. What should a solutions architect do to meet these requirements?",
        "options": [
          {
            "A": "Write individual policies for each S3 bucket to grant read permission for only CloudFront access."
          },
          {
            "B": "Write an S3 bucket policy that assigns the CloudFront distribution ID as the Principal and assigns the target S3 bucket as the Amazon Resource Name (ARN)."
          },
          {
            "C": "Create an origin access identity (OAI). Assign the OAI to the CloudFront distribution. Configure the S3 bucket permissions so that only the OAI has read permission."
          },
          {
            "D": "Create an IAM user. Grant the user read permission to objects in the S3 bucket. Assign the user to CloudFront."
          }
        ],
        "correct_answer": "C",
        "explanation": "The correct answer is C: Create anoriginaccessidentity(OAI). Assignthe OAItothe CloudFront distribution. Configurethe S 3 bucketpermissionssothatonlythe OAIhasread permission.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "51",
        "q_text": "A company wants to ingest customer payment data into the company's data lake in Amazon S3. The company receives payment data every minute on average. The company wants to analyze the payment data in real time. Then the company wants to ingest the data into the data lake. Which solution will meet these requirements with the MOST operational efficiency?",
        "options": [
          {
            "A": "Use Amazon Kinesis Data Firehose to ingest data. Use Amazon Kinesis Data Analytics to analyze the data in real time."
          },
          {
            "B": "Use AWS Glue to ingest data. Use Amazon Kinesis Data Analytics to analyze the data in real time."
          },
          {
            "C": "Use Amazon API Gateway to ingest data. Use AWS Lambda to analyze the data in real time."
          },
          {
            "D": "Use Amazon Kinesis Data Streams to ingest data. Use AWS Lambda to analyze the data in real time."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is A: Use Amazon Kinesis Data Firehosetoingestdata. Use Amazon Kinesis Data Analytics to analyze the data in real time. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "52",
        "q_text": "A company is creating an application that runs on containers in a VPC. The application stores and accesses data in an Amazon S3 bucket. During the development phase, the application will store and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs and wants to prevent traffic from traversing the internet whenever possible. Which solution will meet these requirements?",
        "options": [
          {
            "A": "Enable S3 Transfer Acceleration for the S3 bucket"
          },
          {
            "B": "Create an interface endpoint for Amazon S3 in the VPC. Associate this endpoint with all route tables in the VPC"
          },
          {
            "C": "Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in the VPC. Correct answer"
          },
          {
            "D": "Enable S3 Intelligent-Tiering for the S3 bucket"
          }
        ],
        "correct_answer": "C",
        "explanation": "The correct answer is C: Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in the VPC. Correct answer. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "53",
        "q_text": "An ecommerce company hosts its analytics application in the AWS Cloud. The application generates about 300 MB of data each month. The data is stored in JSON format. The company is evaluating a disaster recovery solution to backup the data. The data must be accessible in milliseconds if it is needed, and the data must be kept for 30 days. Which solution meets these requirements MOST cost-effectively?",
        "options": [
          {
            "A": "Amazon RDS for PostgreSQL"
          },
          {
            "B": "Amazon Open Search Service (Amazon Elasticsearch Service)"
          },
          {
            "C": "Amazon S3 Glacier"
          },
          {
            "D": "Amazon S3 Standard"
          }
        ],
        "correct_answer": "D",
        "explanation": "The correct answer is D .This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "54",
        "q_text": "An ecommerce company has an order-processing application that uses Amazon API Gateway and an AWS Lambda function. The application stores data in an Amazon Aurora PostgreSQL database. During a recent sales event, a sudden surge in customer orders occurred. Some customers experienced timeouts, and the application did not process the orders of those customers. A solutions architect determined that the CPU utilization and memory utilization were high on the database because of a large number of open connections. The solutions architect needs to prevent the timeout errors while making the least possible changes to the application. Which solution will meet these requirements?",
        "options": [
          {
            "A": "Use Amazon RDS Proxy to create a proxy for the database. Modify the Lambda function to use the RDS Proxy endpoint instead of the database endpoint."
          },
          {
            "B": "Create a read replica for the database in a different AWS Region. Use query string parameters in API Gateway to route traffic to the read replica."
          },
          {
            "C": "Configure provisioned concurrency for the Lambda function. Modify the database to be a global database in multiple AWS Regions."
          },
          {
            "D": "Migrate the data from Aurora PostgreSQL to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS). Modify the Lambda function to use the DynamoDB table."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "55",
        "q_text": "A company uses Amazon S3 as its data lake. The company has a new partner that must use SFTP to upload data files. A solutions architect needs to implement a highly available SFTP solution that minimizes operational overhead. Which solution will meet these requirements?",
        "options": [
          {
            "A": "Launch an Amazon EC2 instance in a private subnet in a VPC. Instruct the new partner to upload files to the EC2 instance by using a VPN. Run a cron job script on the EC2 instance to upload files to the S3 data lake."
          },
          {
            "B": "Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible endpoint. Choose the S3 data lake as the destination."
          },
          {
            "C": "Use Amazon S3 File Gateway as an SFTP server. Expose the S3 File Gateway endpoint URL to the new partner. Share the S3 File Gateway endpoint with the new partner."
          },
          {
            "D": "Launch Amazon EC2 instances in a private subnet in a VPC. Place a Network Load Balancer (NLB) in front of the EC2 instances. Create an SFTP listener port for the NLB. Share the NLB hostname with the new partner. Run a cron job script on the EC2 instances to upload files to the S3 data lake."
          }
        ],
        "correct_answer": "B",
        "explanation": "The correct answer is B: Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible endpoint. Choose the S3 data lake as the destination. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "56",
        "q_text": "A company has multiple VPCs across AWS Regions to support and run workloads that are isolated from workloads in other Regions. Because of a recent application launch requirement, the company\u2019s VPCs must communicate with all other VPCs across all Regions. Which solution will meet these requirements with the LEAST amount of administrative effort?",
        "options": [
          {
            "A": "Use AWS Transit Gateway to manage VPC communication in a single Region and Transit Gateway peering across Regions to manage VPC communications."
          },
          {
            "B": "Use AWS Private Link across all Regions to connect VPCs across Regions and manage VPC communications"
          },
          {
            "C": "Use AWS Direct Connect gateways across all Regions to connect VPCs across regions and manage VPC communications."
          },
          {
            "D": "Use VPC peering to manage VPC communication in a single Region. Use VPC peering across Regions to manage VPC communications."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "57",
        "q_text": "A company has a web application for travel ticketing. The application is based on a database that runs in a single data center in North America. The company wants to expand the application to serve a global user base. The company needs to deploy the application to multiple AWS Regions. Average latency must be less than 1 second on updates to the reservation database. The company wants to have separate deployments of its web platform across multiple Regions. However, the company must maintain a single primary reservation database that is globally consistent. Which solution should a solutions architect recommend to meet these requirements?",
        "options": [
          {
            "A": "Migrate the application to an Amazon Aurora Serverless database. Deploy instances of the database to each Region. Use the correct Regional endpoint in each Regional deployment to access the database. Use AWS Lambda functions to process event streams in each Region to synchronize the databases."
          },
          {
            "B": "Convert the application to use Amazon Dynamo DB. Use a global table for the center reservation table. Use the correct Regional endpoint in each Regional deployment."
          },
          {
            "C": "Migrate the database to an Amazon Aurora MySQL database. Deploy Aurora Read Replicas in each Region. Use the correct Regional endpoint in each Regional deployment for access to the database."
          },
          {
            "D": "Migrate the database to an Amazon RDS for MySQL database. Deploy MySQL read replicas in each Region. Use the correct Regional endpoint in each Regional deployment for access to the database."
          }
        ],
        "correct_answer": "B",
        "explanation": "The correct answer is B: Convert the application to use Amazon Dynamo DB. Use a global table for the center reservation table. Use the correct Regional endpoint in each Regional deployment. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "58",
        "q_text": "A company has multiple Windows file servers on premises. The company wants to migrate and consolidate its files into an Amazon FSx for Windows File Server file system. File permissions must be preserved to ensure that access rights do not change. Which solutions will meet these requirements? (Choose two.)",
        "options": [
          {
            "A": "Order an AWS Snowball Edge Storage Optimized device. Connect the device to the on-premises network. Copy data to the device by using the AWS CLI. Ship the device back to AWS for import into Amazon S3. Schedule AWS Data Sync tasks to transfer the data to the FSx for Windows File Server filesystem."
          },
          {
            "B": "Deploy AWS Data Sync agents on premises. Schedule Data Sync tasks to transfer the data to the FSx for Windows File Server filesystem. Correct selection"
          },
          {
            "C": "Remove the drives from each file server. Ship the drives to AWS for import into Amazon S3. Schedule AWS Data Sync tasks to transfer the data to the FSx for Windows File Server filesystem."
          },
          {
            "D": "Order an AWS Snowcone device. Connect the device to the on-premises network. Launch AWS Data Sync agents on the device. Schedule Data Sync tasks to transfer the data to the FSx for Windows File Server filesystem."
          },
          {
            "E": "Copy the shares on each file server into Amazon S3 buckets by using the AWS CLI. Schedule AWS Data Sync tasks to transfer the data to the FSx for Windows File Server filesystem."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "59",
        "q_text": "A company's website provides users with downloadable historical performance reports. The website needs a solution that will scale to meet the company's website demands globally. The solution should be cost-effective, limit the provisioning of infrastructure resources, and provide the fastest possible response time. Which combination should a solutions architect recommend to meet these requirements?",
        "options": [
          {
            "A": "Application Load Balancer with Amazon EC2 Auto Scaling"
          },
          {
            "B": "Amazon CloudFront and Amazon S3"
          },
          {
            "C": "AWS Lambda and Amazon Dynamo DB"
          },
          {
            "D": "Amazon Route 53 with internal Application Load Balancers"
          }
        ],
        "correct_answer": "B",
        "explanation": "The correct answer is B: Amazon CloudFront and Amazon S3. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "60",
        "q_text": "A company provides an API to its users that automates inquiries for tax computations based on item prices. The company experiences a large number of inquiries during the holiday season only that causes lower response times. A solutions architect needs to design a solution that is scalable and elastic. What should the solutions architect do to accomplish this?",
        "options": [
          {
            "A": "Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon EC2 instance. API Gateway accepts and passes the item names to the EC2 instance for tax computations."
          },
          {
            "B": "Design a REST API using Amazon API Gateway that accepts the item names. API Gateway passes item names to AWS Lambda for tax computations. Correct answer"
          },
          {
            "C": "Create an Application Load Balancer that has two Amazon EC2 instances behind it. The EC2 instances will compute the tax on the received item names."
          },
          {
            "D": "Provide an API hosted on an Amazon EC2 instance. The EC2 instance performs the required computations when the API request is made."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "61",
        "q_text": "A company has a service that reads and writes large amounts of data from an Amazon S3 bucket in the same AWS Region. The service is deployed on Amazon EC2 instances within the private subnet of a VPC. The service communicates with Amazon S3 over a NAT gateway in the public subnet. However, the company wants a solution that will reduce the data output costs. Which solution will meet these requirements MOST cost-effectively?",
        "options": [
          {
            "A": "Provision a dedicated EC2 NAT instance in the private subnet. Configure the route table for the public subnet to use the elastic network interface of this instance as the destination for all S3 traffic."
          },
          {
            "B": "Provision a VPC gateway endpoint. Configure the route table for the private subnet to use the gateway endpoint as the route for all S3 traffic."
          },
          {
            "C": "Provision a second NAT gateway. Configure the route table for the private subnet to use this NAT gateway as the destination for all S3 traffic."
          },
          {
            "D": "Provision a dedicated EC2 NAT instance in the public subnet. Configure the route table for the private subnet to use the elastic network interface of this instance as the destination for all S3 traffic."
          }
        ],
        "correct_answer": "61",
        "explanation": "The correct answer is B: Provision a VPC gateway endpoint. Configure the routetable for the private subnettousethegatewayendpointastherouteforall S 3 traffic. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "63",
        "q_text": "A solutions architect needs to securely store a database username and password that an application uses to access an Amazon RDS DB instance. The application that accesses the database runs on an Amazon EC2 instance. The solutions architect wants to create a secure parameter in AWS Systems Manager Parameter Store. What should the solutions architect do to meet this requirement?",
        "options": [
          {
            "A": "Create an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM role to the EC2 instance. Correct answer"
          },
          {
            "B": "Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance. Specify Amazon RDS as a principal in the trust policy."
          },
          {
            "C": "Create an IAM trust relationship between the DB instance and the EC2 instance. Specify Systems Manager as a principal in the trust policy."
          },
          {
            "D": "Create an IAM policy that allows read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM policy to the EC2 instance."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "64",
        "q_text": "A solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the information submitted by users is sensitive. The application uses HTTPS but needs another layer of security. The sensitive information should be protected throughout the entire application stack, and access to the information should be restricted to certain applications. Which action should the solutions architect take?",
        "options": [
          {
            "A": "Configure a CloudFront field-level encryption profile."
          },
          {
            "B": "Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol Policy."
          },
          {
            "C": "Configure a CloudFront signed cookie."
          },
          {
            "D": "Configure a CloudFront signed URL."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is A: Configure a CloudFront field-level encryption profile. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "65",
        "q_text": "A company has a legacy data processing application that runs on Amazon EC2 instances. Data is processed sequentially, but the order of results does not matter. The application uses a monolithic architecture. The only way that the company can scale the application to meet increased demand is to increase the size of the instances. The company's developers have decided to rewrite the application to use a microservices architecture on Amazon Elastic Container Service (Amazon ECS). What should a solutions architect recommend for communication between the microservices?",
        "options": [
          {
            "A": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Add code to the data producers, and publish notifications to the topic. Add code to the data consumers to subscribe to the topic."
          },
          {
            "B": "Create an Amazon Dynamo DB table. Enable Dynamo DB Streams. Add code to the data producers to insert data into the table. Add code to the data consumers to use the Dynamo DB Streams API to detect new table entries and retrieve the data."
          },
          {
            "C": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Add code to the data producers, and send data to the queue. Add code to the data consumers to process data from the queue."
          },
          {
            "D": "Create an AWS Lambda function to pass messages. Add code to the data producers to call the Lambda function with a data object. Add code to the data consumers to receive a data object that is passed from the Lambda function."
          }
        ],
        "correct_answer": "C",
        "explanation": "The correct answer is C: Create an Amazon Simple Queue Service (Amazon SQS) queue. Add code to the data producers, and send data to the queue. Add code to the data consumers to process data from the queue. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "66",
        "q_text": "A company seeks a storage solution for its application. The solution must be highly available and scalable. The solution also must function as a file system, be mountable by multiple Linux instances in AWS and on-premises through native protocols, and have minimum size requirements. The company has set up a Site-to-Site VPN for access from its on-premises network to its VPC. Which storage solution meets these requirements?",
        "options": [
          {
            "C": "Which storage solution meets these requirements?"
          },
          {
            "A": "Amazon FSx Multi-AZ deployments"
          },
          {
            "B": "Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes"
          },
          {
            "C": "Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points"
          },
          {
            "D": "Amazon Elastic File System (Amazon EFS) with multiple mount targets Correct answer"
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "67",
        "q_text": "A company is developing an e-commerce application that will consist of a load-balanced frontend, a container-based application, and a relational database. A solutions architect needs to create a highly available solution that operates with as little manual intervention as possible. Which solutions meet these requirements? (Choose two)",
        "options": [
          {
            "A": "Create an Amazon RDS DB instance in Multi-AZ mode."
          },
          {
            "B": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type to handle the dynamic application load."
          },
          {
            "C": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type to handle the dynamic application load."
          },
          {
            "D": "Create an Amazon RDS DB instance and one or more replicas in another Availability Zone."
          },
          {
            "E": "Create an Amazon EC2 instance-based Docker cluster to handle the dynamic application load."
          }
        ],
        "correct_answer": "A, B",
        "explanation": "The correct answer is A and E. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "68",
        "q_text": "A company has an ordering application that stores customer information in Amazon RDS for MySQL. During regular business hours, employees run one-time queries for reporting purposes. Timeouts are occurring during order processing because the reporting queries are taking a long time to run. The company needs to solve the timeouts without preventing employees from performing queries. What should a solutions architect do to meet these requirements?",
        "options": [
          {
            "A": "Create a read replica. Distribute the ordering application to the primary DB instance and the read replica."
          },
          {
            "B": "Schedule the reporting queries for non-peak hours."
          },
          {
            "C": "Create a read replica. Move reporting queries to the read replica."
          },
          {
            "D": "Migrate the ordering application to Amazon Dynamo DB with on-demand capacity."
          }
        ],
        "correct_answer": "C",
        "explanation": "The correct answer is C: Create a read replica. Move reporting queries to the read replica. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "69",
        "q_text": "A company has a service that reads and writes large amounts of data from an Amazon S3 bucket in the same AWS Region. The service is deployed on Amazon EC2 instances within the private subnet of a VPC. The service communicates with Amazon S3 over a NAT gateway in the public subnet. However, the company wants a solution that will reduce the data output costs. Which solution will meet theserequirements MOST cost-effectively?",
        "options": [
          {
            "A": "Provision a dedicated EC2 NAT instance in the private subnet. Configure the route table for the public subnet to use the elastic network interface of this instance as the destination for all S3 traffic."
          },
          {
            "B": "Provision a VPC gateway endpoint. Configure the route table for the private subnet to use the gateway endpoint as the route for all S3 traffic."
          },
          {
            "C": "Provision a second NAT gateway. Configure the route table for the private subnet to use this NAT gateway as the destination for all S3 traffic."
          },
          {
            "D": "Provision a dedicated EC2 NAT instance in the public subnet. Configure the route table for the private subnet to use the elastic network interface of this instance as the destination for all S3 traffic."
          }
        ],
        "correct_answer": "B",
        "explanation": "The correct answer is B: Provision a VPC gateway endpoint. Configure the routetable for the private subnettousethegatewayendpointastherouteforall S 3 traffic.. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "70",
        "q_text": "A company has Amazon EC2 instances that run nightly batch jobs to process data. The EC2 instances run in an Auto Scaling group that uses On-Demand billing. If a job fails on one instance, another instance will reprocess the job. The batch jobs run between 12:00 AM and 06:00 AM local time every day. Which solution will provide EC2 instances to meet these requirements MOST cost-effectively?",
        "options": [
          {
            "A": "Purchase a 1-year Reserved Instance for the specific instance type and operating system of the instances in the Auto Scaling group that the batch job uses."
          },
          {
            "B": "Purchase a 1-year Savings Plan for Amazon EC2 that covers the instance family of the Auto Scaling group that the batch job uses."
          },
          {
            "C": "Create a new launch template for the Auto Scaling group. Set the instances to Spot Instances. Set a policy to scale out based on CPU usage."
          },
          {
            "D": "Create a new launch template for the Auto Scaling group. Increase the instance size. Set a policy to scale out based on CPU usage."
          }
        ],
        "correct_answer": "C",
        "explanation": "The correct answer is C: Create a new launch template for the Auto Scaling group. Set the instances to Spot Instances. Set a policy to scale out based on CPU usage. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "71",
        "q_text": "A company's web application is running on Amazon EC2 instances behind an Application Load Balancer. The company recently changed its policy, which now requires the application to be accessed from one specific country only. Which configuration will meet this requirement?",
        "options": [
          {
            "A": "Configure the security group for the EC2 instances."
          },
          {
            "B": "Configure AWS WAF on the Application Load Balancer in a VPC."
          },
          {
            "C": "Configure the network ACL for the subnet that contains the EC2 instances."
          },
          {
            "D": "Configure the security group on the Application Load Balancer."
          }
        ],
        "correct_answer": "B",
        "explanation": "The correct answer is B: Configure AWS WAF on the Application Load Balancer in a VPC. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "72",
        "q_text": "A company runs an infrastructure monitoring service. The company is building a new feature that will enable the service to monitor data in customer AWS accounts. The new feature will call AWS APIs in customer accounts to describe Amazon EC2 instances and read Amazon Cloud Watch metrics. What should the company do to obtain access to customer accounts in the MOST secure way?",
        "options": [
          {
            "A": "Ensure that the customers create an Amazon Cognito user in their account to use an IAM role with read-only EC2 and Cloud Watch permissions. Encrypt and store the Amazon Cognito user and password in a secrets management system."
          },
          {
            "B": "Ensure that the customers create an IAM user in their account with read-only EC2 and Cloud Watch permissions. Encrypt and store customer access and secret keys in a secrets management system."
          },
          {
            "C": "Create a serverless API that implements a token vending machine to provide temporary AWS credentials for a role with read-only EC2 and Cloud Watch permissions."
          },
          {
            "D": "Ensure that the customers create an IAM role in their account with read-only EC2 and Cloud Watch permissions and a trust policy to the company's account."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is : . This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      },
      {
        "q_number": "73",
        "q_text": "A company runs a web-based portal that provides users with global breaking news, local alerts, and weather updates. The portal delivers each user a personalized view by using a mixture of static and dynamic content. Content is served over HTTPS through an API server running on an Amazon EC2 instance behind an Application Load Balancer (ALB). The company wants the portal to provide this content to its users across the world as quickly as possible. How should a solutions architect design the application to ensure the LEAST amount of latency for all users?",
        "options": [
          {
            "A": "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 geolocation routing policy to serve all content from the ALB in the closest Region."
          },
          {
            "B": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin. Correct answer"
          },
          {
            "C": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve the static content. Serve the dynamic content directly from the ALB."
          },
          {
            "D": "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest Region."
          }
        ],
        "correct_answer": "A",
        "explanation": "The correct answer is A. This solution addresses the key requirements outlined in the scenario by leveraging AWS best practices for cloud architecture. The chosen approach provides optimal performance, cost-efficiency, and reliability while maintaining security and compliance standards. Alternative options would either introduce unnecessary complexity, increase operational overhead, or fail to meet specific requirements mentioned in the question. AWS services are designed to work together seamlessly, and this solution demonstrates effective integration of services to solve business challenges. When implementing AWS solutions, it's important to consider factors such as scalability, availability, security, and cost optimization, all of which are addressed by the correct answer."
      }
]